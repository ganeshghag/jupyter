{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48882ec1-1777-4a6c-b4fa-2472eede2ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting xgboost\n  Downloading xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\nCollecting nvidia-nccl-cu12\n  Downloading nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.4 MB)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.7.3)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.21.5)\nInstalling collected packages: nvidia-nccl-cu12, xgboost\nSuccessfully installed nvidia-nccl-cu12-2.25.1 xgboost-2.1.3\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efe3810-34e1-4544-a159-064f5ed27aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%restart_python\n",
    "from pyspark.sql import SparkSession\n",
    "#from sparkxgb import SparkXGBRegressor  # Import XGBoost for PySpark\n",
    "from xgboost.spark import SparkXGBRegressor as sparkxgb\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42b003f-7d78-4085-952d-00bf6b3b5f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- longitude: double (nullable = true)\n |-- latitude: double (nullable = true)\n |-- housing_median_age: double (nullable = true)\n |-- total_rooms: double (nullable = true)\n |-- total_bedrooms: double (nullable = true)\n |-- population: double (nullable = true)\n |-- households: double (nullable = true)\n |-- median_income: double (nullable = true)\n |-- median_house_value: double (nullable = true)\n |-- ocean_proximity: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 07:43:12,669 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with\n\tbooster params: {'objective': 'reg:squarederror', 'device': 'cpu', 'maxDepth': 6, 'eta': 0.1, 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-02-05 07:43:20,320 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 966.74\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ Initialize Spark Session\n",
    "#spark = SparkSession.builder.appName(PySparkXGBoostRegression).getOrCreate()\n",
    "\n",
    "# 2️⃣ Load Dataset (California Housing dataset)\n",
    "#data = spark.read.csv(\"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\",\n",
    "#                      header=True, inferSchema=True)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "#from sparkxgb import SparkXGBRegressor  # Import XGBoost for PySpark\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "data = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"dbfs:/FileStore/tables/housing.csv\")\n",
    "#data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\")\n",
    "# 3️⃣ Check Schema\n",
    "data.printSchema()\n",
    "\n",
    "# 4️⃣ Handle Missing Values (Drop Nulls)\n",
    "data = data.dropna()\n",
    "\n",
    "# 5️⃣ Feature Engineering - Assemble Features into a Single Column\n",
    "feature_columns = data.columns[:-1]  # Exclude target column (median_house_value)\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data).select(\"features\", \"median_house_value\")\n",
    "\n",
    "# 6️⃣ Train-Test Split (80-20)\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 7️⃣ Initialize & Train XGBoost Regressor\n",
    "xgb = SparkXGBRegressor(features_col=\"features\", label_col=\"median_house_value\",\n",
    "                        maxDepth=6, eta=0.1, objective=\"reg:squarederror\")\n",
    "model = xgb.fit(train_data)\n",
    "\n",
    "# 8️⃣ Model Evaluation on Test Data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"median_house_value\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "# 9️⃣ Print Model Performance\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7dd2f91-3b22-4042-bfc5-41cbc5af3b73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n|summary|median_house_value|\n+-------+------------------+\n|  count|             20640|\n|   mean|206855.81690891474|\n| stddev|115395.61587441359|\n|    min|           14999.0|\n|    max|          500001.0|\n+-------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"dbfs:/FileStore/tables/housing.csv\")\n",
    "\n",
    "data.describe([\"median_house_value\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a87065e4-8c78-4143-b51e-fcb8c5e497da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TestRegression",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
